# Elkeval - RAG Testbed & Evaluator

Elkeval is a Streamlit application designed to help you develop, test, and evaluate Retrieval Augmented Generation (RAG) pipelines, particularly those leveraging Elasticsearch.

## Features

* **üè† Homepage**:
    * Displays the Elkeval logo and provides a welcoming overview of the application's capabilities.
    * Guides users on how to get started.

* **üìù Ground Truth Generation**:
    * Upload raw documents (JSON format with a 'content' field).
    * **Language Detection**: Automatically detects the language of the uploaded document.
    * **Prompt Translation**: Translates the English base prompts to the detected document language using an LLM, ensuring culturally and linguistically relevant Q&A generation.
    * Generates question-answer pairs based on document chunks using customizable LLM prompts.
    * Critiques generated Q&A for groundedness, relevance, and standalone quality using LLM-based evaluations.
    * Filters and exports high-quality Q&A pairs for building robust evaluation datasets.

* **üîé RAG Processor**:
    * Interactively test your RAG pipeline with single questions against a configured Elasticsearch index.
    * **Dynamic Index Field Selection**:
        * Fetches and displays available fields from the specified Elasticsearch index.
        * Allows users to select a specific "Lexical Search Field" (text/keyword type) for full-text search components.
        * Allows users to select a specific "Semantic Search Field" (dense_vector or text type for ELSER-like models) for semantic search components.
        * Allows users to select a "Primary Content Field for LLM Prompt" to specify which field's content from retrieved documents is used to build the context for the LLM.
        * Allows users to select a "Content Field for Reranker Input" when using reranking methods.
    * **Multi-Method Selection**: Select one or more retrieval methods (e.g., fulltext, semantic, hybrid, BGE-enhanced, rerankers) to run simultaneously.
    * View retrieved Elasticsearch results and the final LLM-generated answer, clearly grouped by each selected retrieval method for easy comparison.
    * **Batch Processing with Multiple Methods**:
        * Upload a JSON file containing multiple questions.
        * Process the batch using all selected retrieval methods and dynamically chosen fields.
        * Generates separate output JSON files for each retrieval method, prefixed with the method name (e.g., `Semantic_Search_batch_output.json`).
        * Provides download links for each method-specific output file.

* **‚öñÔ∏è RAGAS Evaluation**:
    * Evaluate the outputs of your RAG pipeline (from the RAG Processor or other sources) using established RAGAS metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall).
    * Process multiple RAG output files (JSON format).
    * Save detailed evaluation scores and summaries in a JSON file.

* **üìä Evaluation Visualization**:
    * Upload the JSON summary file generated by the RAGAS Evaluation step.
    * Visualize key RAGAS metrics using violin plots for easy comparison across different RAG experiments or configurations.
    * View summary tables of average scores per document/experiment.
    * **LLM-Powered Explanation**: Get an AI-generated explanation and interpretation of the RAGAS visualization results using a customizable prompt.

* **‚öôÔ∏è Prompt Settings**:
    * View and modify all LLM prompt templates used throughout the application (for Q&A generation, critiques, translation, visualization explanations).
    * Changes are applied for the current user session, allowing for on-the-fly experimentation with prompt engineering.
    * Option to reset prompts to their default values.

## Project Structure

elkeval/‚îú‚îÄ‚îÄ app.py                   # Main Streamlit application‚îú‚îÄ‚îÄ prompts.py               # LLM Prompt templates‚îú‚îÄ‚îÄ retriever_utils.py       # Utilities for Elasticsearch and OpenAI (User-provided)‚îú‚îÄ‚îÄ tabs/                    # Directory for Streamlit tab modules‚îÇ   ‚îú‚îÄ‚îÄ init.py‚îÇ   ‚îú‚îÄ‚îÄ ground_truth_tab.py‚îÇ   ‚îú‚îÄ‚îÄ rag_processor_tab.py‚îÇ   ‚îú‚îÄ‚îÄ ragas_eval_tab.py‚îÇ   ‚îú‚îÄ‚îÄ viz_tab.py‚îÇ   ‚îî‚îÄ‚îÄ prompt_settings_tab.py‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies‚îú‚îÄ‚îÄ Dockerfile               # For building the Docker image‚îú‚îÄ‚îÄ .dockerignore            # Files to ignore for Docker build‚îú‚îÄ‚îÄ .env.example             # Example environment file (users should create their own .env)‚îî‚îÄ‚îÄ README.md                # This file
## Prerequisites

* Python 3.9+
* Access to an Elasticsearch instance
* Azure OpenAI API credentials (or other compatible OpenAI API)
* `langdetect` library for language detection

## Local Setup

1.  **Clone the repository (if applicable):**
    ```bash
    git clone <your-repo-url>
    cd elkeval
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up environment variables:**
    Create a `.env` file in the project root directory. You can copy `.env.example` (if provided) and fill in your actual credentials. It should include variables for Azure OpenAI (API key, base endpoint, deployment names for chat and embedding models, API versions) and Elasticsearch (URL, user, password, default index name). Example:

    ```env
    # Azure OpenAI Main Model
    AZURE_OPENAI_KEY="your_azure_openai_key"
    AZURE_OPENAI_BASE="your_azure_openai_endpoint"
    AZURE_OPENAI_DEPLOYMENT="your_chat_model_deployment_name"
    AZURE_OPENAI_MODEL="your_chat_model_name" 
    AZURE_OPENAI_VERSION="api_version_for_chat_model"

    # Azure OpenAI Embedding Model
    AZURE_API_KEY_EMBEDDING="your_azure_embedding_api_key" 
    AZURE_ENGINE_EMBEDDING="your_embedding_model_deployment_name"
    AZURE_API_VERSION_EMBEDDING="api_version_for_embedding_model"
    # AZURE_EMBEDDING_MODEL="text-embedding-ada-002" # Optional if using default

    # Elasticsearch
    ES_URL="http://localhost:9200"
    ES_USER="your_es_user"         
    ES_PASSWORD="your_es_password" 
    ES_INDEX_NAME="your_default_es_index_from_env" # This will override the hardcoded default in retriever_utils.py
    # ES_CID="your_elastic_cloud_id" # If using Elastic Cloud
    ```
    **Note:** Ensure your `retriever_utils.py` correctly loads these environment variables.

5.  **Run the Streamlit application:**
    ```bash
    streamlit run app.py
    ```
    The application should now be accessible in your web browser, typically at `http://localhost:8501`.

## Docker Setup

1.  **Build the Docker image:**
    From the project root directory (where the `Dockerfile` is located):
    ```bash
    docker build -t elkeval-app .
    ```

2.  **Run the Docker container:**
    You need to provide the environment variables to the container, typically using an env file.

    * **Using an env file:**
        Ensure your `.env` file is present in the project root.
        ```bash
        docker run -p 8501:8501 --env-file .env elkeval-app
        ```

3.  Access the application in your web browser at `http://localhost:8501`.

## Contributing

(Add guidelines if you plan for others to contribute)

## License

(Specify a license if applicable, e.g., MIT, Apache 2.0)
